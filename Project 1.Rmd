---
title: "Project 1 Order Data Analysis"
author: "Amanda, Adil & Brian"
date: "September 29, 2018"
output: 
  html_document:
    keep_md: TRUE
    toc: true
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# XML: What is it and why should we use it? <br>

#### What is XML?
Extensible Markup Language, or XML, is a dynamic language that is similar to, but more flexible than HTML.  XML simplifies data sharing as it provides a flexible way to create information formats and electronically share structured data via the internet or corporate networks.<br> 

XML is self-defining, meaning the structure of data is embedded within the data itself. This feature removes the need to pre-build the structure to store the data when attempting to share it.<br>

The main advantage of XML is its simplicity. It enables you to consolidate large chunks of information into an XML document which provides structure and organization to the information. This feature facilitates the rendering of such data in the website environment.<br>

#### Where is it used? 
XML is useful in data transfer. Attempting to share or exchange data in incompatible formats is time consuming at best. As XML stores data in a plain text format, it is a platform independent way of storing and transporting data.

#### Why is it a good way to store data?
XML is a good way to store complex and highly variable data, hierarchical data, and data in which the the format may change over time. 

You can learn more about XML here:<br>
<https://searchmicroservices.techtarget.com/definition/XML-Extensible-Markup-Language>,<br>
<https://www.w3schools.com/xml/xml_whatis.asp><br>
<https://www.sitepoint.com/really-good-introduction-xml/><br>

# Package to read in XML data
#### What package(s) should be used to read in XML data?
There are several ways and packages that could be used to read in XML data. The 'XML' package has many online tutorials, help, and trouble shooting resources available. This, along with the fact that it contains many functions that make it easy to access and convert your data make it our recommendation for reading in XML data.

# Reading in data and performing some sample analysis

#### Loading libraries
```{r, warning = FALSE, message = FALSE}
library(tidyverse)
#requiring more packages that will be helpful later to handle XML data
if (!require(XML)) {install.packages("XML")}
library(XML)
library(methods)
if (!require(plyr)) {install.packages("plyr")}
require(plyr)
if (!require(dplyr)) {install.packages("dplyr")}
require(dplyr)
if (!require(lubridate)) {install.packages("lubridate")}
library(lubridate)
library(ggplot2)
```


#### Reading in XML Data

We found a website with many public XML datasets, some of them contain very interesting information.<br>
[XML data repository](http://aiweb.cs.washington.edu/research/projects/xmltk/xmldata/www/repository.html)<br>
Mondial is an XML dataset from World geographic database integrated from the CIA World Factbook, the International Atlas, and the TERRA database among other sources. The data is made public and downloadable from washington.edu datasets repository online.<br>

However,Mondial has inconsistent number of nodes and/or subnodes throughout the XML content structure. It seems that we can't use standard protocols to fix those. We need to do some ad hoc commands to make that work. As a result we are switching to another more structured dataset called orders from the same repository, with which we had success transforming the XML data in R. Please see the code below. We commented out the lines for Mondial in case we want to challenge ourselves later.

```{r Mondial}
# mondialParse <- xmlParse("http://aiweb.cs.washington.edu/research/projects/xmltk/xmldata/data/mondial/mondial-3.0.xml", useInternalNodes = TRUE)
# class(mondialParse)
# mondialParse

orderParse <- xmlParse("http://aiweb.cs.washington.edu/research/projects/xmltk/xmldata/data/tpc-h/orders.xml")
class(orderParse)
```

#### Transforming XML into a Dataframe
```{r xmlToDF}
# xmlInList <- xmlToList(mondialParse)
# monialDF <- ldply(xmlInList, data.frame)
# Can't use the above methodology because number of nodes and subnodes are not consistent throughout the entire XML structure.

# monialDF <- xmlToDataFrame("http://aiweb.cs.washington.edu/research/projects/xmltk/xmldata/data/mondial/mondial-3.0.xml")
# This does not work either.

xmlInList <- xmlToList(orderParse)
orderDF <- ldply(xmlInList, data.frame)
orderTB <- tbl_df(orderDF)
str(orderTB)
```
The two quantitative variables are total price and the full order date. The two categorical variables we are going to explore are order priority and order status. For a concise breakdown between quantitative vs categorical variables, please refer to:<br> 
<https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/tables/supporting-topics/basics/categorical-and-quantitative-variables/><br>

#### Data Wrangling
All columns of data are parsed into factors, so we need to convert them to character or numeric before manipulating them.
```{r Conversions}
# Remove the last attr row (with NAs) from XML and make a copy so that orderTB is intact
orderTB <- orderTB[-15001,]
orderTB_copy <- orderTB

# start converting columns
orderTB_copy$O_TOTALPRICE <- as.numeric(levels(orderTB$O_TOTALPRICE))[orderTB$O_TOTALPRICE]
orderTB_copy$O_ORDERDATE <- as.character(levels(orderTB$O_ORDERDATE))[orderTB$O_ORDERDATE]

# If f is a factor, as.numeric(levels(f))[f] is more efficient than as.numeric(as.character(f)) according to R documentation. That is what we used above.

# Convert order date from string to date type
d <- orderTB$O_ORDERDATE
parsedDate <- as.Date(d,"%Y-%m-%d")

# Check if date conversion is successful
class(parsedDate)

orderTB_copy$O_ORDERDATE <- parsedDate

str(orderTB_copy)
```

#### Variable Creation
Create a variable to indicate which season the order was made. Use the definition of seasons from the northern hemisphere.
```{r Creation}

m <- month(orderTB_copy$O_ORDERDATE)

# Use ifelse for vectorized operation
Season <- ifelse(m >= 3 & m <= 5, "Spring", ifelse(m >= 6 & m <= 8, "Summer", ifelse(m >= 9 & m <= 11, "Autumn", "Winter")))

# Convert Season into a factor with defined levels (so that the seasons are sorted properly in the aggregation)
orderTB_copy$Season <- factor(Season, levels = c("Spring", "Summer", "Autumn", "Winter"))
orderTB_copy$Year <- as.integer(year(orderTB_copy$O_ORDERDATE))

# Reorder O_Order.Priority
orderTB_copy$O_ORDER.PRIORITY <- ordered(orderTB_copy$O_ORDER.PRIORITY, levels = c("5-LOW","4-NOT SPECIFIED", "3-MEDIUM", "2-HIGH", "1-URGENT"))
```

#### Aggregation Summary
Use the order date and the newly created season variable to aggregate some data; mean and median by year/season, and standard deviation by season.
```{r Summaries}
# Since dplyr and plyr are invoked together, we need to specify dplyr::group_by and dplyr::summarize. Otherwise, we will get only 1 row of summary.
sumOrderData <- orderTB_copy %>% dplyr::group_by(Year, Season) %>% dplyr::summarize(meanPrice = mean(O_TOTALPRICE, na.rm = TRUE), medianPrice = median(O_TOTALPRICE, na.rm = TRUE), sdPrice = sd(O_TOTALPRICE, na.rm = TRUE))
sumOrderData

# Looking at standard deviation by season using tapply from base package.
tapply(X=orderTB_copy$O_TOTALPRICE, INDEX = orderTB_copy$Season, FUN = sd)

# In addition to Year/season, we try to summarise by order status/season.
sumOrderData2 <- orderTB_copy %>% dplyr::group_by(O_ORDERSTATUS, Season) %>% dplyr::mutate(meanPrice = mean(O_TOTALPRICE, na.rm = TRUE), medianPrice = median(O_TOTALPRICE, na.rm = TRUE), sdPrice = sd(O_TOTALPRICE, na.rm = TRUE))
sumOrderData2
```

#### Bar Plots
```{r}
# Bar chart of transcation count by year and season
bar1 <- ggplot(orderTB_copy, aes(x = Year))
bar1 + geom_bar(aes(fill = Season)) + labs(x = "Year", y = "Transaction Counts", title = "Transaction Record from 1992 - 1998")

# Bar chart of median order price by order status and season
bar3 <- ggplot(sumOrderData2, aes(x= O_ORDERSTATUS, y = medianPrice))
bar3 + geom_bar(aes(fill=Season), stat = "identity", position = "dodge") + labs(x = "Order Status", y = "Median Order Price", title = "Bar Chart of Median Order Price by Order Status and Season")

```

#### Scatter Plots with Coloring
```{r Scatter}
scatter <- ggplot(orderTB_copy, aes(x = O_ORDERDATE, y = O_TOTALPRICE)) 
scatter + geom_point(aes(col = Season)) + labs(x = "Date", y = "Total Order Price per Customer", title = "Transaction Record from 1992 - 1998", color = "Seasons")

#This plot is too dense to gain valuable insight.  We will try a line plot with a summary measure to see if we can more easily identify trends.
```


```{r Line}
l <- ggplot(sumOrderData, aes(x = Year, y = meanPrice, color = Season))
l + geom_line(lwd = 1.2) + labs(x = "Year", y = "Average Order Price", title = "Transaction trend from 2002 - 2008", color = "Seasons")
```

#### Box Plots
```{r Box1}
b1 <- ggplot(orderTB_copy, aes(x = as.factor(Year), y = O_TOTALPRICE))  + geom_boxplot(fill = "pink") + labs(x = "Year", y = "Total Order Price per Customer", title = "Boxplot of Total Order Price by year")
b1
```

```{r Box2}
b2 <- ggplot(orderTB_copy, aes(x = as.factor(Year), y = O_TOTALPRICE, fill = Season))  + geom_boxplot() + labs(x = "Year", y = "Total Order Price per Customer", title = "Boxplot of Total Order Price by Year and Season")
b2

#Bar chart of median order price by year and season

g<-ggplot(sumOrderData, aes(x= Year, y = medianPrice))

```


#### Frequency count tables for other categorical variables
```{r Frequency}
#Frequency table
table(orderTB_copy$O_ORDER.PRIORITY, orderTB_copy$Season)
```

#### Create a custom function
We created a function that calculates how much money a specific customer has spent with the company and the date range in which this occured.
```{r, error=TRUE}
getTotalperCust<-function(df, variable){
  # if statement and a stop/error to validate that the customer entered is valid
  if(!(variable%in%df$O_CUSTKEY)){
    stop("Customer does not exist,please try again with a valid customerKey")
  }
  
  tot<- df %>% filter(O_CUSTKEY == variable) %>% dplyr::summarise(totPercust = sum(O_TOTALPRICE))
  minyear<- df %>% filter(O_CUSTKEY == variable) %>% summarise(miny<-min(Year))
  maxyear<- df %>% filter(O_CUSTKEY == variable) %>% summarise(maxy<-max(Year))
  
  return(list(TotalMoneySpent = paste0("Customer ", variable, " has spent $", tot, " between ", minyear, " and ", maxyear, ".")))
}
# Checking how many observations the data set has for Cusomer number 1
customerNum1 <- filter(orderTB_copy, O_CUSTKEY == 1)
customerNum1 # 9 Observations

getTotalperCust(orderTB_copy, 1110)
getTotalperCust(orderTB_copy, 1369)
getTotalperCust(orderTB_copy, 730)
```


# By Group-A: second example of reading in XML data set, and do analysis
## Read in XML data set
```{r message=FALSE, cache=TRUE}
#We noticed some issues in using a URL with an https epithet. We had to load the URL using RCurl to change the security settings.

library(RCurl)
ajaxURL<-"https://www4.stat.ncsu.edu/~post/558/datasets/ajax.xml"
ajaxFetch<-getURL(ajaxURL)
ajaxData<-xmlTreeParse(ajaxFetch,useInternal=TRUE)
```

Now that we have the data in our environment in raw format, we have to clean it up
```{r cache=TRUE}
#We have to unlist since we don't have a square matrix

xmlInList2 <- xmlToList(ajaxData)
ajaxDF <- ldply(unlist(xmlInList2), data.frame)
ajaxTB <- tbl_df(ajaxDF)
```

Now the entire dataset is in a long form with redundant variable names, so we want to change the values in the id colume into columns names, which means convert the long form dataset into a wide form.
```{r cache=TRUE}
library(tidyr)
ajaxTB$X..i..<-as.character(ajaxTB$X..i..)
ajaxTBwide<-ajaxTB %>% group_by(.id) %>% #group by everything other than the value column
  dplyr::mutate(row_id=1:n()) %>% # build group index
  ungroup() %>% spread(key=.id, value=X..i..) %>% # spread
  select(-row_id)  # drop the index
```

```{r}
#Clunky way to rename variables to get rid of collection.poi beginning portion
names(ajaxTBwide)<-sub("\\w{10}","",names(ajaxTBwide)) #removes "collection"
names(ajaxTBwide)<-sub(".","",names(ajaxTBwide)) #removes "." 
names(ajaxTBwide)<-sub("\\w{3}","",names(ajaxTBwide))  #removes "poi"
names(ajaxTBwide)<-sub(".","",names(ajaxTBwide)) #removes "."

#Cleans data to change all Y/N variants to 0/1
ajaxTBwide[ajaxTBwide=="y"]<-1
ajaxTBwide[ajaxTBwide=="Y"]<-1
ajaxTBwide[ajaxTBwide=="Yes"]<-1
ajaxTBwide[ajaxTBwide=="n"]<-0
ajaxTBwide[ajaxTBwide=="N"]<-0
ajaxTBwide[ajaxTBwide=="No"]<-0

#Changes variables from chr to numeric for mean calculations for the 0/1 variables
ajaxTBwide$brueggers<-as.numeric(ajaxTBwide$brueggers)
ajaxTBwide$coffeeshop<-as.numeric(ajaxTBwide$coffeeshop)
ajaxTBwide$drivethru<-as.numeric(ajaxTBwide$drivethru)
ajaxTBwide$hide_employment<-as.numeric(ajaxTBwide$hide_employment)
ajaxTBwide$loyalty<-as.numeric(ajaxTBwide$loyalty)
ajaxTBwide$meetingrooms<-as.numeric(ajaxTBwide$meetingrooms)
ajaxTBwide$wifi<-as.numeric(ajaxTBwide$wifi)
```

## Exploratory data analysis
(Zhen-When I run this code, there is some error)
After cleaning up the data a little bit, we will do some sample analysis.

#summaries
ajaxTBwide%>%group_by(wifi)%>%summarize(grocery,identity)


### Numeric Summaries:
Numeric summary of the proportion of Caribou coffee locations that have wifi for each state.
```{r}
wifiPropData<-ajaxTBwide %>% group_by(state) %>% summarise(wifiProp=mean(wifi,na.rm=TRUE))
```

Numeric summary of the proportion of Caribou coffee locations that have drivethru for each state.
```{r}
driveThruPropData<-ajaxTBwide %>% group_by(state) %>% summarise(driveThruProp=mean(drivethru,na.rm=TRUE))
```

### Create a bar plot:
Bar plot showing proportion of Caribou coffee locations that have wifi for each state.
```{r}
p<-ggplot(wifiPropData, aes(x=state, y=wifiProp))
p+geom_bar(stat="identity", fill="blue")+
labs(x="States", title="Barplot of proportion of Caribou coffee locations that have wifi for each state")+
ylab("Proportion of wifi covered")+
theme(axis.text.x=element_text(angle=90))
```


### Create a scatter plot:
Scatter plot showing proportion of Caribou coffee locations that have drivethru for each state...
```{r}
g<-ggplot(driveThruPropData, aes(x = state, y = driveThruProp))
g+geom_point()+
labs(x="States", title="Scatter plot of proportion of Caribou coffee locations that have drivethru for each state")+
ylab("Proportion of locations that have drivethru")+
theme(axis.text.x=element_text(angle=90))
```

(Zhen-we may add more summarizations, and plots)

### Create a map of locations in the United States
```{r}
library(mapdata)
usa <- map_data("usa")
mapdf <- filter(ajaxTBwide, (as.numeric(ajaxTBwide$collection.poi.longitude) > -130) & (as.numeric(ajaxTBwide$collection.poi.longitude) < -65))
ggplot() + geom_polygon(data = usa, aes(x=long, y = lat, group = group)) + coord_fixed(1.3)
cariboumap <- ggplot() + 
  geom_polygon(data = usa, aes(x=long, y = lat, group = group), fill = "#008b8b", color = "gray") + 
  coord_fixed(1.3)
cariboumap + 
  geom_point(data = mapdf, aes(x = (as.numeric(mapdf$collection.poi.longitude)), y = (as.numeric(mapdf$collection.poi.latitude))), color = "black", size = 0.5) 
```
